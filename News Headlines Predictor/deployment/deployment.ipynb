{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deployment.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1KcqyxoAqKJQzoTESqgpKwD9P8mX3aivs","authorship_tag":"ABX9TyNTnLS9WwTi307lrzHOscOW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"z5Q-qxl8XRUf"},"source":["We will use streamlit library for designing our UI and ngrok for pipelining the components and delivering them to the UI . The UI has option of 4 different classifiers, and you can predict the sentiment by selecting each of them. Further training of these classifiers is still underway. Also, the UI provides a visualization dashboard wherein you can analyse some key statistics about the text in the corpus."]},{"cell_type":"code","metadata":{"id":"hmsloxq8PiWR","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1619947392139,"user_tz":-330,"elapsed":20575,"user":{"displayName":"Sagar Sinha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisbAHtYEeRBI2qGqLInEKxoqEdHwswW1wxLa_L=s64","userId":"15681381308865956446"}},"outputId":"a1ed98e8-2d5c-4f63-9e17-c38f06fa1a16"},"source":["#No need to reinstall it if you have, already...\n","!pip install streamlit\n","!pip install pyngrok"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting streamlit\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/98/4725661dc5719c05ba7e3f9744407ce91e2d982cb6c9601de2bbb62e2dd0/streamlit-0.81.0-py2.py3-none-any.whl (8.2MB)\n","\u001b[K     |████████████████████████████████| 8.2MB 6.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n","Collecting base58\n","  Downloading https://files.pythonhosted.org/packages/b8/a1/d9f565e9910c09fd325dc638765e8843a19fa696275c16cc08cf3b0a3c25/base58-2.1.0-py3-none-any.whl\n","Collecting pydeck>=0.1.dev5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/bc/f0e44828e4290367c869591d50d3671a4d0ee94926da6cb734b7b200308c/pydeck-0.6.2-py2.py3-none-any.whl (4.2MB)\n","\u001b[K     |████████████████████████████████| 4.2MB 52.2MB/s \n","\u001b[?25hRequirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.12.4)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n","Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n","Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.1)\n","Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n","Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n","Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n","Collecting blinker\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB)\n","\u001b[K     |████████████████████████████████| 112kB 44.3MB/s \n","\u001b[?25hCollecting gitpython\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n","\u001b[K     |████████████████████████████████| 163kB 42.1MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (20.9)\n","Collecting validators\n","  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n","Collecting watchdog; platform_system != \"Darwin\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/b2/b4ebe23174fd00ec94ac3f58ebf85f1090c49858feab1ca62ed7ea4d2f2f/watchdog-2.0.3-py3-none-manylinux2014_x86_64.whl (74kB)\n","\u001b[K     |████████████████████████████████| 81kB 6.9MB/s \n","\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n","Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n","Collecting ipykernel>=5.1.2; python_version >= \"3.4\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/7d/9f8ac1b1b76f2f1538b5650f0b5636bae082724b1e06939a3a9d38e1380e/ipykernel-5.5.3-py3-none-any.whl (120kB)\n","\u001b[K     |████████████████████████████████| 122kB 45.7MB/s \n","\u001b[?25hRequirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (2.11.3)\n","Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (56.0.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2020.12.5)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tzlocal->streamlit) (2018.9)\n","Collecting gitdb<5,>=4.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 7.5MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n","Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (4.4.2)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n","Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.5.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10.1->pydeck>=0.1.dev5->streamlit) (1.1.1)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n","Collecting smmap<5,>=3.0.1\n","  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.7.1)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.8.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.6.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.18)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.5)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (22.0.3)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.9.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.5)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n","Building wheels for collected packages: blinker\n","  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for blinker: filename=blinker-1.4-cp37-none-any.whl size=13448 sha256=05f09b1b373216250d7006f71cad73cfa4abb1bfffe56fe21916bfac01f9f920\n","  Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89\n","Successfully built blinker\n","\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.5.3 which is incompatible.\u001b[0m\n","Installing collected packages: base58, ipykernel, pydeck, blinker, smmap, gitdb, gitpython, validators, watchdog, streamlit\n","  Found existing installation: ipykernel 4.10.1\n","    Uninstalling ipykernel-4.10.1:\n","      Successfully uninstalled ipykernel-4.10.1\n","Successfully installed base58-2.1.0 blinker-1.4 gitdb-4.0.7 gitpython-3.1.14 ipykernel-5.5.3 pydeck-0.6.2 smmap-4.0.0 streamlit-0.81.0 validators-0.18.2 watchdog-2.0.3\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["ipykernel"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting pyngrok\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/4e/a2fe095bbe17cf26424c4abcd22a0490e22d01cc628f25af5e220ddbf6f0/pyngrok-5.0.5.tar.gz (745kB)\n","\u001b[K     |████████████████████████████████| 747kB 5.0MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n","Building wheels for collected packages: pyngrok\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-5.0.5-cp37-none-any.whl size=19246 sha256=607883beb9ca821a0f506a664d625b125fdbcf15296e0a6dc61dec43328e9ca8\n","  Stored in directory: /root/.cache/pip/wheels/0c/13/64/5ebbcc22eaf53fdf5766b397c1fb17c83f5775fdccf0ea1b88\n","Successfully built pyngrok\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-5.0.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nmBZZ8L8Xsuf","executionInfo":{"status":"ok","timestamp":1619947396844,"user_tz":-330,"elapsed":1165,"user":{"displayName":"Sagar Sinha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisbAHtYEeRBI2qGqLInEKxoqEdHwswW1wxLa_L=s64","userId":"15681381308865956446"}},"outputId":"b0cfcec7-fb09-41ad-ccb6-f809423eb4eb"},"source":["%%writefile app.py\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import streamlit as st\n","from wordcloud import WordCloud, STOPWORDS\n","\n","#Install the requisite libraries\n","import nltk #Text preprocessing library\n","import joblib #For saving and loading ML models\n","import keras #For loading saved models\n","\n","#Downloading the relevant libraries and dependencies in NLTK module for preprocessing\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","#Regular Expressions\n","import re\n","\n","#Text to numerical features - ML algorithms\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","#Initialising the stemmer and lemmatizer\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","\n","#Pre-defining the vocabulary size to be 10000, sentence \n","vocab_size = 10000\n","sent_length = 25\n","embedding_vector_features = 40\n","\n","#Text preprocessing\n","from keras.preprocessing.text import Tokenizer, one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","\n","\n","#Load the saved models\n","#Loading Random Forest Classifier\n","rf_classifier = joblib.load('/content/drive/MyDrive/Datasets/Indian Financial News Headlines/src/models/saved-models-vectorizer/finalized_ht.sav')\n","\n","#Loading Multinomial Byes Classifier\n","mnb_classifier = joblib.load('/content/drive/MyDrive/Datasets/Indian Financial News Headlines/src/models/saved-models-vectorizer/finalized_mnb.sav')\n","\n","#Loading Keras basic RNN model\n","rnn_base_classifier = keras.models.load_model('/content/drive/MyDrive/Datasets/Indian Financial News Headlines/src/models/saved-models-vectorizer/RNN_basic')\n","\n","#Loading ANN model\n","ann_base_classifier = keras.models.load_model('/content/drive/MyDrive/Datasets/Indian Financial News Headlines/src/models/saved-models-vectorizer/NN_basic')\n","\n","#Loading the vectorizer\n","tfv = joblib.load('/content/drive/MyDrive/Datasets/Indian Financial News Headlines/src/models/saved-models-vectorizer/finalized_tfv.sav')\n","\n","#Load the  dataset for performing visualizations\n","vis_data = pd.read_csv('/content/drive/MyDrive/Datasets/Indian Financial News Headlines/data/processed/processed_data.csv')\n","vis_turney = pd.read_csv('/content/drive/MyDrive/Datasets/Indian Financial News Headlines/data/processed/turney2.csv')\n","vis_data['Date'] = pd.to_datetime(vis_data['Date'], infer_datetime_format=True)\n","vis_data['Year'] = vis_data['Date'].dt.year\n","vis_data_sorted = vis_data.sort_values(by='Year', ascending=False)\n","vis_data_sorted.drop(['Unnamed: 0'], axis=1, inplace=True)\n","\n","#Custom Test Prediction, for checks\n","def clean_raw(text):\n","   new_review = str(text)\n","   new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n","   new_review = new_review.lower()\n","   new_review = new_review.split()\n","   all_stopwords = stopwords.words('english')\n","   new_review = [lemmatizer.lemmatize(word) for word in new_review]\n","   new_review = ' '.join(new_review)\n","\n","   return new_review\n","\n","def transformer_tf(classifier, text, vectorizer):\n","   nn_cnt, adj_cnt, adv_cnt, vrb_cnt, mean_word_len = 0, 0, 0, 0, 0 \n","\n","   new_review = str(text)\n","   new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n","   new_review = new_review.lower()\n","   new_review = new_review.split()\n","\n","   new_review = [lemmatizer.lemmatize(word) for word in new_review]\n","   tagged_text = nltk.pos_tag(new_review)\n","\n","   new_review = ' '.join(new_review)\n","   new_corpus = [new_review]\n","\n","\n","\n","   #Transforming the corpus to array of features\n","   text_features = vectorizer.transform(new_corpus).toarray()\n","\n","   #Calculating mean word length\n","   mean_word_length = np.mean([len(word) for word in new_corpus])\n","  \n","   #Counting labels \n","   for (word, tag) in tagged_text:\n","      if tag=='NN' or tag=='NNS' or tag=='NNP' or tag=='NNS':\n","         nn_cnt += 1\n","\n","      elif tag=='JJ' or tag=='JJR' or tag=='JJS':\n","         adj_cnt += 1\n","\n","      elif tag=='RB' or tag=='RBR' or tag=='RBS':\n","         adv_cnt += 1\n","\n","      elif tag=='VB' or tag=='VBD' or tag=='VBG' or tag=='VBN' or tag=='VBP':\n","         vrb_cnt += 1\n","\n","   #Constructing a pandas dataframe to store numerical features and extract them later for concatenation\n","   data = {'Noun Count' : nn_cnt, 'Adverb Count' : adv_cnt, 'Adjective Count' : adj_cnt, 'Verb Count' : vrb_cnt, 'Mean Length' : mean_word_length}\n","   df = pd.DataFrame(data)\n","   df_values = df.values\n","\n","   #Stack the feature vectors together\n","   feature_stack = hstack(text_features, df_values).tocsr()\n","\n","   #Label Prediction\n","   new_y_pred = classifier.predict(feature_stack)\n","\n","   opinion = \" \"\n","   if new_y_pred[0] == 0:\n","     opinion = \"Negative statement or no opinion\"\n","   else:\n","     opinion = \"Positive opinion\"\n","\n","   return opinion\n","\n","def transformer_oh(text):\n","   cleaned_text = clean_raw(text)\n","   oh_encoded_text = [one_hot(cleaned_text, vocab_size)]\n","   embedded_encoded_text = pad_sequences(oh_encoded_text, padding='post', maxlen=sent_length)\n","   return embedded_encoded_text\n","\n","def get_imp(bow, mf, ngram1, ngram2):\n","   cvt = CountVectorizer(bow, ngram_range=(ngram1, ngram2), max_features=mf,stop_words='english')\n","   matrix=cvt.fit_transform(bow)\n","   return pd.Series(np.array(matrix.sum(axis=0))[0], index=cvt.get_feature_names()).sort_values(ascending=False).head(100)\n","\n","def random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n","    h = 180\n","    s = int(100.0 * 255.0 / 255.0)\n","    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n","    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n","\n","def wordcloud_generator(data):\n","   wordcloud = WordCloud(\n","                  background_color='white',\n","                  stopwords=STOPWORDS,\n","                  max_words=200,\n","                  max_font_size=40, \n","                  random_state=42).generate(str(data['Combined_Text']))\n","   return wordcloud\n","\n","\n","   \n","\n","PAGE_CONFIG = {\"page_title\":\"StColab.io\",\"page_icon\":\":smiley:\",\"layout\":\"centered\"}\n","st.set_option('deprecation.showPyplotGlobalUse', False)\n","st.set_page_config(**PAGE_CONFIG)\n","\n","def main():\n","\n","   header_text = ''' \n","                  <h3 style=\"text-align:center; text-transform:uppercase; text-decoration:none; letter-spacing:2px;\">\n","                  Sentiment Analysis of Financial News Headlines\n","                  \n","\n","                 '''\n","\n","   st.markdown(header_text, unsafe_allow_html=True)\n","\n","   menu = ['Classification','Visualization']\n","   st.sidebar.subheader(\"Choose to predict or visualize\")\n","   choice = st.sidebar.selectbox(\"Click the desired option\", menu)\n","\n","   if choice == 'Classification':\n","\n","\n","\n","      st.markdown('''\n","                  Enter a news headline in the text area corresponding to a stock or company\n","                  \n","                  ''')\n","      \n","      menupred = ['Random Forest Classifier', 'Multinomial Naive Byes', 'ANN-basic', 'RNN-basic']\n","      st.sidebar.subheader(\"Choose your classifier\")\n","      predchoice = st.sidebar.selectbox(\"Click the desired option\", menupred)\n","\n","      if predchoice == 'Random Forest Classifier':\n","         st.success(\"You have successfully selected the {} classifier\".format(predchoice))\n","         with st.beta_container():\n","            text = st.text_area('Enter text', height=100)\n","            button = st.button(\"Predict\")\n","            classifier = rf_classifier\n","            \n","            if button and len(text) != 0:\n","              pred_text = transformer_tf(classifier, text, tfv)\n","              st.write(pred_text)\n","\n","            if len(text) == 0:\n","              st.write(\"Please enter some valid text\")\n","\n","\n","      elif predchoice == 'Multinomial Naive Byes':\n","         st.success(\"You have successfully selected the {} classifier\".format(predchoice))\n","         with st.beta_container():\n","            text = st.text_area('Enter text', height=100)\n","            button = st.button(\"Predict\")\n","            classifier = mnb_classifier\n","            \n","            if button and len(text) != 0:\n","                pred_text = transformer_tf(classifer, text, tfv)\n","                st.write(pred_text)\n","\n","            if len(text) == 0:\n","                st.write(\"Please enter some valid text\")\n","\n","\n","\n","      elif predchoice == 'Sequence Model':\n","         st.success(\"You have successfully selected the {} classifier\".format(predchoice))\n","         with st.beta_container():\n","            text = st.text_area('Enter text', height=100)\n","            button = st.button(\"Predict\")\n","            classifier = rnn_base_classifier\n","            oh_encoded_text = transformer_oh(text)\n","\n","            if button and len(text) != 0:\n","               pred_label = classifier.predict_classes(oh_encoded_text)\n","\n","\n","               \n","               st.write(pred_label)\n","              #  if int(pred_label) == 1:\n","              #     st.write(\"WELL, HAVE FAITH AND ALL WILL BE FINE!\")\n","\n","              #  else:\n","              #     st.write(\"WELL, ALL IS NOT WELL OR SEEMINGLY NEUTRAL TO TAKE A STAND\")\n","\n","            if len(text) == 0:\n","               st.write(\"PLEASE ENTER SOME VALID TEXT\")\n","            \n","            else:\n","               pass\n","\n","\n","\n","      elif predchoice == 'ANN model':\n","         st.success(\"You have successfully selected the {} classifier\".format(predchoice))\n","         with st.beta_container():\n","            text = st.text_area('Enter text', height=100)\n","            button = st.button(\"Predict\")\n","            classifier = ann_base_classifier\n","            oh_encoded_text = transformer_oh(text)\n","            \n","            if button and len(text) != 0:\n","               pred_label = classifier.predict_classes(oh_encoded_text)\n","\n","\n","\n","               st.write(pred_label)\n","               \n","               if int(pred_label[0]) == 1:\n","                  st.write(\"WELL, HAVE FAITH AND ALL WILL BE FINE!\")\n","\n","               else:\n","                  st.write(\"WELL, ALL IS NOT WELL OR SEEMINGLY NEUTRAL TO TAKE A STAND\")\n","\n","            if len(text) == 0:\n","               st.write(\"PLEASE ENTER SOME VALID TEXT\")\n","            \n","            else:\n","               pass\n","\n","\n","   elif choice == 'Visualization':\n","      st.markdown(\"Here are some interesting and fun visualizations we get on the processed dataset\")\n","      st.markdown(''' \n","      \n","      \n","                  ''')\n","      #Display the first 5 rows of the processed dataset\n","\n","      #Extracting the corpus\n","      corpus = vis_data['Combined_Text']\n","      bow = vis_data_sorted['Combined_Text']\n","\n","      \n","      #The code for following visualizations - mention the inspirations later\n","\n","      #Storing important unigrams in 1-year gap\n","      total_data_unigram = get_imp(bow.tolist(), 5000, ngram1=1, ngram2=1)\n","      imp_unigrams = {}\n","      for year in vis_data_sorted['Year'].unique():\n","         _bow = vis_data_sorted[vis_data_sorted['Year'] == year]['Combined_Text'].tolist()\n","         imp_unigrams[year] = get_imp(_bow, mf=5000, ngram1=1, ngram2=1)\n","\n","\n","      #Storing common unigrams in 1-year gap\n","      com_unigrams = {}\n","      for year in np.arange(2014, 2020, 1):\n","        if year == 2020:\n","            com_unigrams[year] = set(imp_unigrams[year].index).intersection(set(imp_unigrams[year-1].index))\n","\n","        else:\n","            com_unigrams[year] = set(imp_unigrams[year].index).intersection(set(imp_unigrams[year+1].index))\n","\n","\n","      #Storing important bigrams in 1-year gap\n","      total_data_bigram = get_imp(bow.tolist(), 5000, ngram1=2, ngram2=2)\n","      imp_bigrams = {}\n","      for year in vis_data_sorted['Year'].unique():\n","         _bow = vis_data_sorted[vis_data_sorted['Year'] == year]['Combined_Text'].tolist()\n","         imp_bigrams[year] = get_imp(_bow, mf=5000, ngram1=2, ngram2=2)\n","\n","\n","      #Storing common bigrams in 1-year gap\n","      com_bigrams = {}\n","      for year in np.arange(2014, 2020, 1):\n","         if year == 2020:\n","            com_bigrams[year] = set(imp_bigrams[year].index).intersection(set(imp_bigrams[year-1].index))\n","\n","         else:\n","            com_bigrams[year] = set(imp_bigrams[year].index).intersection(set(imp_bigrams[year+1].index))\n","\n","\n","      #Storing important trigrams in 1-year gap\n","      total_data_trigram = get_imp(bow.tolist(), 5000, ngram1=3, ngram2=3)\n","      imp_trigrams = {}\n","      for year in vis_data_sorted['Year'].unique():\n","         _bow = vis_data_sorted[vis_data_sorted['Year'] == year]['Combined_Text'].tolist()\n","         imp_trigrams[year] = get_imp(_bow, mf=5000, ngram1=3, ngram2=3)\n","\n","\n","      #Storing common trigrams in 1-year gap\n","      com_trigrams = {}\n","      for year in np.arange(2014, 2020, 1):\n","         if year == 2020:\n","            com_trigrams[year] = set(imp_trigrams[year].index).intersection(set(imp_trigrams[year-1].index))\n","\n","         else:\n","            com_trigrams[year] = set(imp_trigrams[year].index).intersection(set(imp_trigrams[year+1].index))\n","      \n","\n","      st.markdown('''\n","      \n","                  ''')\n","      st.title('Plot of Unigrams')\n","      st.bar_chart(total_data_unigram.head(20))\n","\n","      st.title('Plot of Bigrams')\n","      st.bar_chart(total_data_bigram.head(20))\n","\n","      st.title('Plot of Trigrams')\n","      st.bar_chart(total_data_trigram.head(20))\n","\n","      st.markdown('''\n","      \n","                  ''')\n","      st.title(\"Plot of Year-Wise distribution of most frequent n-grams\")\n","\n","      st.markdown('''\n","      \n","                  ''')\n","      \n","\n","      \n","      #Initialising the slider\n","      value = st.sidebar.slider('Choose year for corresponding n-gram visualization', \n","                                min_value = 2014, max_value = 2020, value = 2015, step = 1)\n","\n","      st.markdown('Unigrams for {} year'.format(value))\n","      st.bar_chart(imp_unigrams[value].head(5))\n","\n","      st.markdown('Bigrams for {} year'.format(value))\n","      st.bar_chart(imp_bigrams[value].head(5))\n","\n","      st.markdown('Trigrams for {} year'.format(value))\n","      st.bar_chart(imp_trigrams[value].head(5))\n","      \n","\n","      #Initialising another dropdown\n","      st.markdown(\n","          '''\n","\n","          ''' \n","      )\n","      st.title('WordCloud Visualization')\n","      menu_bank = ['Hdfc','Axis', 'RBI', 'Yes']\n","      value = st.sidebar.selectbox('Word Cloud Visualization', menu_bank)\n","\n","      if value == 'Hdfc':\n","         index_hdfc = vis_data_sorted['Combined_Text'].str.match(r'(?=.*\\bhdfc\\b)(?=.*\\bbank\\b).*$', case=False)\n","         data_hdfc = vis_data_sorted.loc[index_hdfc]\n","         wordcloud = wordcloud_generator(data_hdfc)\n","         plt.imshow(wordcloud)\n","         plt.axis('off')\n","         plt.xticks([])\n","         plt.yticks([])\n","         plt.show()\n","         st.pyplot()\n","\n","      elif value == 'Axis':\n","         index_axis = vis_data_sorted['Combined_Text'].str.match(r'(?=.*\\bAxis\\b)(?=.*\\bbank\\b).*$', case=False)\n","         data_axis = vis_data_sorted.loc[index_axis]\n","         wordcloud = wordcloud_generator(data_axis)\n","         plt.imshow(wordcloud)\n","         plt.axis('off')\n","         plt.xticks([])\n","         plt.yticks([])\n","         plt.show()\n","         st.pyplot()\n","\n","\n","      elif value == 'RBI':\n","         index_RBI = vis_data_sorted['Combined_Text'].str.match(r'.*\\bRBI\\b.*$', case=False)\n","         data_RBI = vis_data_sorted.loc[index_RBI]\n","         wordcloud = wordcloud_generator(data_RBI)\n","         plt.imshow(wordcloud)\n","         plt.axis('off')\n","         plt.xticks([])\n","         plt.yticks([])\n","         plt.show()\n","         st.pyplot()\n","\n","\n","      elif value == 'Yes':\n","         index_yes = vis_data_sorted['Combined_Text'].str.match(r'(?=.*\\bYes\\b)(?=.*\\bbank\\b).*$', case=False)\n","         data_yes = vis_data_sorted.loc[index_yes]\n","         wordcloud = wordcloud_generator(data_yes)\n","         plt.imshow(wordcloud)\n","         plt.axis('off')\n","         plt.xticks([])\n","         plt.yticks([])\n","         plt.show()\n","         st.pyplot()\n","\n","\n","      # col1, col2, col3 = st.beta_columns(3)\n","\n","      # with col1:\n","        #  st.header(\"Unigrams\")\n","        #st.bar_chart(total_data_unigram.head(20))\n","        #  total_data_unigram.head(20).plot(kind='bar', figsize=(25, 10), colormap='Set1')\n","        #  plt.xlabel('Unigrams')\n","        #  plt.ylabel('Frequency')\n","        #  plt.title('Count of Unigrams in Dataset', fontsize=20)\n","        #  plt.xticks(size=20)\n","        #  st.pyplot()\n","\n","      # with col2:\n","        #  st.header(\"Bigrams\")\n","        #st.bar_chart(total_data_unigram.head(20))\n","        #  total_data_bigram.head(20).plot(kind='bar', figsize=(25, 10), colormap='Set2')\n","        #  plt.xlabel('Unigrams')\n","        #  plt.ylabel('Frequency')\n","        #  plt.title('Count of Unigrams in Dataset', fontsize=20)\n","        #  plt.xticks(size=20)\n","        #  st.pyplot()\n","\n","      # with col3:\n","        #  st.header(\"Trigrams\")\n","        #st.bar_chart(total_data_unigram.head(20))\n","        #  total_data_unigram.head(20).plot(kind='bar', figsize=(25, 10), colormap='Set3')\n","        #  plt.xlabel('Unigrams')\n","        #  plt.ylabel('Frequency')\n","        #  plt.title('Count of Unigrams in Dataset', fontsize=20)\n","        #  plt.xticks(size=20)\n","        #  st.pyplot()\n","      \n","\n","if __name__ == '__main__':\n","  main()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Overwriting app.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"utWSOFBZYYBS","executionInfo":{"status":"ok","timestamp":1619947691661,"user_tz":-330,"elapsed":2492,"user":{"displayName":"Sagar Sinha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisbAHtYEeRBI2qGqLInEKxoqEdHwswW1wxLa_L=s64","userId":"15681381308865956446"}},"outputId":"c1bda4bd-5c42-4fc9-ad32-11a69c8ddf2e"},"source":["#Auth-token verification\n","!ngrok authtoken 1rFUyukXrkkm7NohUiJZLWUP6EJ_84sEEqtDfFcUcn9JxY78f"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HC69_04mZIYG","executionInfo":{"status":"ok","timestamp":1619949044888,"user_tz":-330,"elapsed":1346942,"user":{"displayName":"Sagar Sinha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisbAHtYEeRBI2qGqLInEKxoqEdHwswW1wxLa_L=s64","userId":"15681381308865956446"}},"outputId":"29b80cbb-d475-415d-e922-fbe55c25987b"},"source":["#Setting up and running the local server on port number 80\n","!streamlit run --server.port 80 app.py >/dev/null "],"execution_count":6,"outputs":[{"output_type":"stream","text":["2021-05-02 09:28:19.811 An update to the [server] config option section was detected. To have these changes be reflected, please restart streamlit.\n","2021-05-02 09:28:22.334108: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","2021-05-02 09:28:28.997260: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2021-05-02 09:28:29.029683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n","2021-05-02 09:28:29.113703: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2021-05-02 09:28:29.113765: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ae97b09996eb): /proc/driver/nvidia/version does not exist\n","2021-05-02 09:28:29.114478: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2021-05-02 09:29:09.404 NumExpr defaulting to 2 threads.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GQMdO6CbWMS","executionInfo":{"status":"ok","timestamp":1619947695317,"user_tz":-330,"elapsed":1087,"user":{"displayName":"Sagar Sinha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisbAHtYEeRBI2qGqLInEKxoqEdHwswW1wxLa_L=s64","userId":"15681381308865956446"}},"outputId":"a64cefee-4382-49ce-dc08-8ffcc8e7eba6"},"source":["from pyngrok import ngrok\n","\n","#Setup a tunnel to the streamlit port 8501\n","public_url = ngrok.connect(port='8501')\n","public_url"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<NgrokTunnel: \"http://731e6d7776cc.ngrok.io\" -> \"http://localhost:80\">"]},"metadata":{"tags":[]},"execution_count":5}]}]}